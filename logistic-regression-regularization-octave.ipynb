{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 2: Logistic Regression\n",
    "%\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the second part\n",
    "%  of the exercise which covers regularization with logistic regression.\n",
    "%\n",
    "%  You will need to complete the following functions in this exericse:\n",
    "%\n",
    "%     sigmoid.m\n",
    "%     costFunction.m\n",
    "%     predict.m\n",
    "%     costFunctionReg.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "%clear ; close all; clc\n",
    "\n",
    "%% Load Data\n",
    "%  The first two columns contains the X values and the third column\n",
    "%  contains the label (y).\n",
    "\n",
    "data = load('exercises/ex2/ex2data2.txt');\n",
    "X = data(:, [1, 2]); y = data(:, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%plotData(X, y);\n",
    "\n",
    "% Put some labels \n",
    "%hold on;\n",
    "\n",
    "% Labels and Legend\n",
    "%xlabel('Microchip Test 1')\n",
    "%ylabel('Microchip Test 2')\n",
    "\n",
    "% Specified in plot order\n",
    "%legend('y = 1', 'y = 0')\n",
    "%hold off;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Regularized Logistic Regression\n",
    "In this part, you are given a dataset with data points that are not\n",
    "linearly separable. However, you would still like to use logistic \n",
    "regression to classify the data points.\n",
    "\n",
    "To do so, you introduce more features to use -- in particular, you add\n",
    "polynomial features to our data matrix (similar to polynomial\n",
    "regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function out = mapFeature(X1, X2)\n",
    "    % MAPFEATURE Feature mapping function to polynomial features\n",
    "    %\n",
    "    %   MAPFEATURE(X1, X2) maps the two input features\n",
    "    %   to quadratic features used in the regularization exercise.\n",
    "    %\n",
    "    %   Returns a new feature array with more features, comprising of\n",
    "    %   X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n",
    "    %\n",
    "    %   Inputs X1, X2 must be the same size\n",
    "    %\n",
    "\n",
    "    degree = 6;\n",
    "    out = ones(size(X1(:,1)));\n",
    "    for i = 1:degree\n",
    "        for j = 0:i\n",
    "            out(:, end+1) = (X1.^(i-j)).*(X2.^j);\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "end\n",
    "\n",
    "% size(X)\n",
    "\n",
    "% Note that mapFeature also adds a column of ones for us, so the intercept term is handled\n",
    "X = mapFeature(X(:,1), X(:,2));\n",
    "\n",
    "% size(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost =  0.69315\n",
      "grad =\n",
      "\n",
      "   8.4746e-03\n",
      "   1.8788e-02\n",
      "   7.7771e-05\n",
      "   5.0345e-02\n",
      "   1.1501e-02\n",
      "   3.7665e-02\n",
      "   1.8356e-02\n",
      "   7.3239e-03\n",
      "   8.1924e-03\n",
      "   2.3476e-02\n",
      "   3.9349e-02\n",
      "   2.2392e-03\n",
      "   1.2860e-02\n",
      "   3.0959e-03\n",
      "   3.9303e-02\n",
      "   1.9971e-02\n",
      "   4.3298e-03\n",
      "   3.3864e-03\n",
      "   5.8382e-03\n",
      "   4.4763e-03\n",
      "   3.1008e-02\n",
      "   3.1031e-02\n",
      "   1.0974e-03\n",
      "   6.3157e-03\n",
      "   4.0850e-04\n",
      "   7.2650e-03\n",
      "   1.3765e-03\n",
      "   3.8794e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% first sigmoid function\n",
    "function g = sigmoid(z)\n",
    "    %SIGMOID Compute sigmoid functoon\n",
    "    %   J = SIGMOID(z) computes the sigmoid of z.\n",
    "\n",
    "    % You need to return the following variables correctly\n",
    "\n",
    "    g = zeros(size(z));\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Compute the sigmoid of each value of z (z can be a matrix,\n",
    "    %               vector or scalar).\n",
    "\n",
    "\n",
    "    for i = 1:rows(z)\n",
    "        for j = 1:columns(z)\n",
    "            g(i,j) = 1 / (1 + e^-z(i,j));\n",
    "        end\n",
    "    end    \n",
    "\n",
    "    #z(1,1)\n",
    "\n",
    "\n",
    "    % =============================================================\n",
    "end\n",
    "\n",
    "function [J, grad] = costFunctionReg(theta, X, y, lambda)\n",
    "    %COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization\n",
    "    %   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using\n",
    "    %   theta as the parameter for regularized logistic regression and the\n",
    "    %   gradient of the cost w.r.t. to the parameters.\n",
    "\n",
    "    % Initialize some useful values\n",
    "    m = length(y); % number of training examples\n",
    "\n",
    "    % You need to return the following variables correctly\n",
    "    J = 0;\n",
    "    grad = zeros(size(theta));\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Compute the cost of a particular choice of theta.\n",
    "    %               You should set J to the cost.\n",
    "    %               Compute the partial derivatives and set grad to the partial\n",
    "    %               derivatives of the cost w.r.t. each parameter in theta\n",
    "\n",
    "\n",
    "    XOK = X(:, 2:end);\n",
    "    thetaOK = theta(2:end,:);\n",
    "\n",
    "    %h = sigmoid(XOK*thetaOK);\n",
    "    h = sigmoid(X*theta);\n",
    "    %J = ( -y'*log(h) - (1-y)'*log(1-h) ) / m + lambda * theta.^2 / 2*m;\n",
    "    J = ( -y'*log(h) - (1-y)'*log(1-h) ) / m;\n",
    "\n",
    "    grad(1,:) = X(:,1)'*(h - y) / m;\n",
    "    grad(2:end,:) = XOK'*(h - y) / m + lambda*thetaOK / m;\n",
    "\n",
    "    % =============================================================\n",
    "\n",
    "end\n",
    "\n",
    "% Initialize fitting parameters\n",
    "initial_theta = zeros(size(X, 2), 1);\n",
    "\n",
    "% Set regularization parameter lambda to 1\n",
    "lambda = 1;\n",
    "\n",
    "%size(initial_theta(2:end,:))\n",
    "%size(X(:, 2:end))\n",
    "\n",
    "%size(y)\n",
    "% Compute and display initial cost and gradient for regularized logistic\n",
    "% regression\n",
    "[cost, grad] = costFunctionReg(initial_theta, X, y, lambda);\n",
    "\n",
    "cost\n",
    "grad\n",
    "\n",
    "%fprintf('Cost at initial theta (zeros): %f\\n', cost);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regularization and Accuracies\n",
    "**Optional Exercise**:\n",
    "In this part, you will get to try different values of lambda and \n",
    "see how regularization affects the decision coundart\n",
    "\n",
    "Try the following values of lambda (0, 1, 10, 100).\n",
    "\n",
    "How does the decision boundary change when you vary lambda? How does\n",
    "the training set accuracy vary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% Initialize fitting parameters\n",
    "initial_theta = zeros(size(X, 2), 1);\n",
    "\n",
    "% Set regularization parameter lambda to 1 (you should vary this)\n",
    "lambda = 1;\n",
    "\n",
    "% Set Options\n",
    "options = optimset('GradObj', 'on', 'MaxIter', 400);\n",
    "\n",
    "% Optimize\n",
    "[theta, J, exit_flag] = ...\n",
    "\tfminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);\n",
    "\n",
    "% Plot Boundary\n",
    "%plotDecisionBoundary(theta, X, y);\n",
    "%hold on;\n",
    "%title(sprintf('lambda = %g', lambda))\n",
    "\n",
    "% Labels and Legend\n",
    "%xlabel('Microchip Test 1')\n",
    "%ylabel('Microchip Test 2')\n",
    "\n",
    "%legend('y = 1', 'y = 0', 'Decision boundary')\n",
    "%hold off;\n",
    "\n",
    "% Compute accuracy on our training set\n",
    "p = predict(theta, X);\n",
    "\n",
    "fprintf('Train Accuracy: %f\\n', mean(double(p == y)) * 100);\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "0.16.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
