{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "Formal definition by Tom M. Mitchell:\n",
    "> A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n",
    "\n",
    "Machine is not initially (or ever) perfect at performing the given task so the function that the machine comes up with for performing the task is called a **hypothesis function**. Another function is used to train the machine, to improve the hypothesis function. As the machine is training, the performance of each prediction attempt can be measured with a **cost function**.\n",
    "\n",
    "Hypothesis function is often denoted as $h_\\theta(x)$, where as the cost function is $J(\\theta_0,\\theta_1)$\n",
    "\n",
    "**Gradient descent** is one training algorithm. It uses cost function to specify such theta values that the cost function reaches minimum (best theta values).\n",
    "\n",
    "$$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "In simpler form:\n",
    "\n",
    "$$\\theta_j:=\\theta_j-\\alpha[Slope\\,of\\,the\\,cost\\,function]$$\n",
    "\n",
    "\n",
    "# Linear regression\n",
    "\n",
    "General form of the **hyphothesis function** in Linear regression:\n",
    "\n",
    "$$\\hat{y}=h_\\theta(x)=\\theta_0+\\theta_1x$$\n",
    "\n",
    "Values of $\\theta_0$ and $\\theta_1$ are modified as the machine learns.\n",
    "\n",
    "**Cost function**:\n",
    "\n",
    "$$J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum_{i=1}^m(\\hat{y}_i-y_i)^2 = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x_i)-y_i)^2$$\n",
    "\n",
    "**Gradient descent**:\n",
    "$$\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x_i)-y_i)$$\n",
    "\n",
    "$$\\theta_1:=\\theta_1-\\alpha\\frac{1}{m}\\sum^m_{i=1}((h_\\theta(x_i)-y_i)x_i)$$\n",
    "\n",
    "Above examples represent linear regression with only one variable x (univariate linear regression). For linear regression with multible variables (multivariate linear regression) the hypothesis function looks like this:\n",
    "$$h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_3+...+\\theta_nx_n$$\n",
    "\n",
    "Using matrix multiplication this can be represented as:\n",
    "$$\n",
    "    h_\\theta(x)=\n",
    "    \\begin{bmatrix}\n",
    "    \\theta_0 & \\theta_1 & \\ldots & \\theta_n \\\\\n",
    "    \\end{bmatrix}        \n",
    "    \\begin{bmatrix}\n",
    "    x_0 \\\\\n",
    "    x_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\end{bmatrix}\n",
    "    =\\theta^Tx\n",
    "$$\n",
    "\n",
    "We can assume $x_0=1$, which makes matrices of training inputs and theta same size, so they can be multiplied together:\n",
    "$$\n",
    "    X=\n",
    "    \\begin{bmatrix}\n",
    "    x_0 \\\\\n",
    "    x_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    x_n \\\\    \n",
    "    \\end{bmatrix}\n",
    "    ,\\,\n",
    "    \\theta=\n",
    "    \\begin{bmatrix}\n",
    "    \\theta_0 \\\\\n",
    "    \\theta_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_n \\\\\n",
    "    \\end{bmatrix}    \n",
    "$$\n",
    "\n",
    "**Vectorized cost function**:\n",
    "\n",
    "$$J(\\theta) = \\dfrac {1}{2m} (X\\theta - \\vec{y})^{T} (X\\theta - \\vec{y})$$\n",
    "\n",
    "Where $\\vec{y}$ denotes the vector of all y values.\n",
    "\n",
    "**Vectorised gradient descent**:\n",
    "\n",
    "$$\\theta := \\theta - \\frac{\\alpha}{m} X^{T} (X\\theta - \\vec{y})$$\n",
    "\n",
    "**Feature normalization** can be used to transform input values so that they are on the same scale. This improves efficiency of the gradient descent algorithm. Feature normalization can be done by using two techniques together: **Mean normalization** subtracts the average input value $\\mu_i$, **feature scaling** divides by mean value or range (eg. max - min) $s_i$.\n",
    "\n",
    "$$x_i := \\dfrac{x_i - \\mu_i}{s_i}$$\n",
    "\n",
    "Where $Î¼_i$ is the average of all the values for feature (i) and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation.\n",
    "\n",
    "Amount of features can be increased (if feasible) by producing new features from current features.\n",
    "\n",
    "Hypothesis function can also be transformed from a straight line (function of degree 1) into a curved line (eg. function of 2 or more degrees or square root function) if feasible. This is called **Polynomial regression**. For example:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1^3$$\n",
    "\n",
    "Polynomial regression increases the range of input values (exponentially) so feature scaling becomes very important.\n",
    "\n",
    "**Normal equation** is a method of finding the optimum theta without iteration.\n",
    "\n",
    "$$\\theta = (X^T X)^{-1}X^T y$$\n",
    "\n",
    "| **Gradient Descent**       | **Normal Equation**                      |\n",
    "| -------------------------- | ---------------------------------------- |\n",
    "| Need to choose alpha       | No need to choose alpha                  |\n",
    "| Needs many iterations      | No need to iterate                       |\n",
    "| O (kn2)                    | O (n3), need to calculate inverse of XTX |\n",
    "| Works well when n is large | Slow if n is very large (over 10,000)    |\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
