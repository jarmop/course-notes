{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class\n",
    "%  Exercise 5 | Regularized Linear Regression and Bias-Variance\n",
    "%\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the\n",
    "%  exercise. You will need to complete the following functions:\n",
    "%\n",
    "%     linearRegCostFunction.m\n",
    "%     learningCurve.m\n",
    "%     validationCurve.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "%clear ; close all; clc\n",
    "\n",
    "%% =========== Part 1: Loading and Visualizing Data =============\n",
    "%  We start the exercise by first loading and visualizing the dataset. \n",
    "%  The following code will load the dataset into your environment and plot\n",
    "%  the data.\n",
    "%\n",
    "\n",
    "% Load Training Data\n",
    "%fprintf('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "% Load from ex5data1: \n",
    "% You will have X, y, Xval, yval, Xtest, ytest in your environment\n",
    "load ('exercises/ex5/ex5data1.mat');\n",
    "\n",
    "% m = Number of examples\n",
    "m = size(X, 1);\n",
    "\n",
    "% Plot training data\n",
    "%plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);\n",
    "%xlabel('Change in water level (x)');\n",
    "%ylabel('Water flowing out of the dam (y)');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg =  0.041667\n",
      "J =  303.99\n",
      "Cost at theta = [1 ; 1]: 303.993192 \n",
      "(this value should be about 303.993192)\n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 2: Regularized Linear Regression Cost =============\n",
    "%  You should now implement the cost function for regularized linear \n",
    "%  regression. \n",
    "%\n",
    "function [J, grad] = linearRegCostFunction(X, y, theta, lambda)\n",
    "    %LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear\n",
    "    %regression with multiple variables\n",
    "    %   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the\n",
    "    %   cost of using theta as the parameter for linear regression to fit the\n",
    "    %   data points in X and y. Returns the cost in J and the gradient in grad\n",
    "\n",
    "    % Initialize some useful values\n",
    "    m = length(y); % number of training examples\n",
    "\n",
    "    % You need to return the following variables correctly\n",
    "    J = 0;\n",
    "    grad = zeros(size(theta));\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Compute the cost and gradient of regularized linear\n",
    "    %               regression for a particular choice of theta.\n",
    "    %\n",
    "    %               You should set J to the cost and grad to the gradient.\n",
    "    %\n",
    "\n",
    "    %theta\n",
    "    %X\n",
    "    %y\n",
    "    \n",
    "    error = X * theta - y;\n",
    "    reg = lambda * sum(theta(2:end,:) .^ 2) / (2 * m);\n",
    "    J = error' * error / (2 * m) + reg;\n",
    "\n",
    "    % =========================================================================\n",
    "\n",
    "    grad = grad(:);\n",
    "\n",
    "end\n",
    "\n",
    "theta = [1 ; 1];\n",
    "J = linearRegCostFunction([ones(m, 1) X], y, theta, 1);\n",
    "J\n",
    "\n",
    "fprintf(['Cost at theta = [1 ; 1]: %f \\n(this value should be about 303.993192)\\n'], J);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% =========== Part 3: Regularized Linear Regression Gradient =============\n",
    "%  You should now implement the gradient for regularized linear \n",
    "%  regression.\n",
    "%\n",
    "\n",
    "theta = [1 ; 1];\n",
    "[J, grad] = linearRegCostFunction([ones(m, 1) X], y, theta, 1);\n",
    "\n",
    "fprintf(['Gradient at theta = [1 ; 1]:  [%f; %f] \\n(this value should be about [-15.303016; 598.250744])\\n'], grad(1), grad(2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% =========== Part 4: Train Linear Regression =============\n",
    "%  Once you have implemented the cost and gradient correctly, the\n",
    "%  trainLinearReg function will use your cost function to train \n",
    "%  regularized linear regression.\n",
    "% \n",
    "%  Write Up Note: The data is non-linear, so this will not give a great \n",
    "%                 fit.\n",
    "%\n",
    "\n",
    "%  Train linear regression with lambda = 0\n",
    "lambda = 0;\n",
    "[theta] = trainLinearReg([ones(m, 1) X], y, lambda);\n",
    "\n",
    "%  Plot fit over the data\n",
    "%plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);\n",
    "%xlabel('Change in water level (x)');\n",
    "%ylabel('Water flowing out of the dam (y)');\n",
    "%hold on;\n",
    "%plot(X, [ones(m, 1) X]*theta, '--', 'LineWidth', 2)\n",
    "%hold off;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% =========== Part 5: Learning Curve for Linear Regression =============\n",
    "%  Next, you should implement the learningCurve function. \n",
    "%\n",
    "%  Write Up Note: Since the model is underfitting the data, we expect to\n",
    "%                 see a graph with \"high bias\" -- slide 8 in ML-advice.pdf \n",
    "%\n",
    "\n",
    "lambda = 0;\n",
    "[error_train, error_val] = learningCurve([ones(m, 1) X], y, [ones(size(Xval, 1), 1) Xval], yval, lambda);\n",
    "\n",
    "%plot(1:m, error_train, 1:m, error_val);\n",
    "%title('Learning curve for linear regression')\n",
    "%legend('Train', 'Cross Validation')\n",
    "%xlabel('Number of training examples')\n",
    "%ylabel('Error')\n",
    "%axis([0 13 0 150])\n",
    "\n",
    "fprintf('# Training Examples\\tTrain Error\\tCross Validation Error\\n');\n",
    "for i = 1:m\n",
    "    fprintf('  \\t%d\\t\\t%f\\t%f\\n', i, error_train(i), error_val(i));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% =========== Part 6: Feature Mapping for Polynomial Regression =============\n",
    "%  One solution to this is to use polynomial regression. You should now\n",
    "%  complete polyFeatures to map each example into its powers\n",
    "%\n",
    "\n",
    "p = 8;\n",
    "\n",
    "% Map X onto Polynomial Features and Normalize\n",
    "X_poly = polyFeatures(X, p);\n",
    "[X_poly, mu, sigma] = featureNormalize(X_poly);  % Normalize\n",
    "X_poly = [ones(m, 1), X_poly];                   % Add Ones\n",
    "\n",
    "% Map X_poly_test and normalize (using mu and sigma)\n",
    "X_poly_test = polyFeatures(Xtest, p);\n",
    "X_poly_test = bsxfun(@minus, X_poly_test, mu);\n",
    "X_poly_test = bsxfun(@rdivide, X_poly_test, sigma);\n",
    "X_poly_test = [ones(size(X_poly_test, 1), 1), X_poly_test];         % Add Ones\n",
    "\n",
    "% Map X_poly_val and normalize (using mu and sigma)\n",
    "X_poly_val = polyFeatures(Xval, p);\n",
    "X_poly_val = bsxfun(@minus, X_poly_val, mu);\n",
    "X_poly_val = bsxfun(@rdivide, X_poly_val, sigma);\n",
    "X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];           % Add Ones\n",
    "\n",
    "fprintf('Normalized Training Example 1:\\n');\n",
    "fprintf('  %f  \\n', X_poly(1, :));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% =========== Part 7: Learning Curve for Polynomial Regression =============\n",
    "%  Now, you will get to experiment with polynomial regression with multiple\n",
    "%  values of lambda. The code below runs polynomial regression with \n",
    "%  lambda = 0. You should try running the code with different values of\n",
    "%  lambda to see how the fit and learning curve change.\n",
    "%\n",
    "\n",
    "lambda = 0;\n",
    "[theta] = trainLinearReg(X_poly, y, lambda);\n",
    "\n",
    "% Plot training data and fit\n",
    "%figure(1);\n",
    "%plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);\n",
    "%plotFit(min(X), max(X), mu, sigma, theta, p);\n",
    "%xlabel('Change in water level (x)');\n",
    "%ylabel('Water flowing out of the dam (y)');\n",
    "%title (sprintf('Polynomial Regression Fit (lambda = %f)', lambda));\n",
    "%\n",
    "%figure(2);\n",
    "\n",
    "[error_train, error_val] = learningCurve(X_poly, y, X_poly_val, yval, lambda);\n",
    "    \n",
    "%plot(1:m, error_train, 1:m, error_val);\n",
    "%title(sprintf('Polynomial Regression Learning Curve (lambda = %f)', lambda));\n",
    "%xlabel('Number of training examples')\n",
    "%ylabel('Error')\n",
    "%axis([0 13 0 100])\n",
    "%legend('Train', 'Cross Validation')\n",
    "\n",
    "fprintf('Polynomial Regression (lambda = %f)\\n\\n', lambda);\n",
    "fprintf('# Training Examples\\tTrain Error\\tCross Validation Error\\n');\n",
    "for i = 1:m\n",
    "    fprintf('  \\t%d\\t\\t%f\\t%f\\n', i, error_train(i), error_val(i));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% =========== Part 8: Validation for Selecting Lambda =============\n",
    "%  You will now implement validationCurve to test various values of \n",
    "%  lambda on a validation set. You will then use this to select the\n",
    "%  \"best\" lambda value.\n",
    "%\n",
    "\n",
    "[lambda_vec, error_train, error_val] = validationCurve(X_poly, y, X_poly_val, yval);\n",
    "\n",
    "%close all;\n",
    "%plot(lambda_vec, error_train, lambda_vec, error_val);\n",
    "%legend('Train', 'Cross Validation');\n",
    "%xlabel('lambda');\n",
    "%ylabel('Error');\n",
    "\n",
    "fprintf('lambda\\t\\tTrain Error\\tValidation Error\\n');\n",
    "for i = 1:length(lambda_vec)\n",
    "\tfprintf(' %f\\t%f\\t%f\\n', ...\n",
    "            lambda_vec(i), error_train(i), error_val(i));\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "0.16.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
