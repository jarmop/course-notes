{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: load: '/Users/jarmo/development/projects/stanford-machine-learning-course/exercises/ex5/ex5data1.mat' found by searching load path\r\n"
     ]
    }
   ],
   "source": [
    "%% Machine Learning Online Class\n",
    "%  Exercise 5 | Regularized Linear Regression and Bias-Variance\n",
    "%\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the\n",
    "%  exercise. You will need to complete the following functions:\n",
    "%\n",
    "%     linearRegCostFunction.m\n",
    "%     learningCurve.m\n",
    "%     validationCurve.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "%clear ; close all; clc\n",
    "\n",
    "%% =========== Part 1: Loading and Visualizing Data =============\n",
    "%  We start the exercise by first loading and visualizing the dataset. \n",
    "%  The following code will load the dataset into your environment and plot\n",
    "%  the data.\n",
    "%\n",
    "\n",
    "% Load Training Data\n",
    "%fprintf('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "% Load from ex5data1: \n",
    "% You will have X, y, Xval, yval, Xtest, ytest in your environment\n",
    "\n",
    "addpath (\"exercises/ex5\");\n",
    "load ('ex5data1.mat');\n",
    "\n",
    "% m = Number of examples\n",
    "m = size(X, 1);\n",
    "\n",
    "% Plot training data\n",
    "%plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);\n",
    "%xlabel('Change in water level (x)');\n",
    "%ylabel('Water flowing out of the dam (y)');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J =  303.99\n",
      "grad =\n",
      "\n",
      "   -15.303\n",
      "   598.251\n",
      "\n",
      "Gradient at theta = [1 ; 1]:  [-15.303016; 598.250744] \n",
      "(this value should be about [-15.303016; 598.250744])\n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 2: Regularized Linear Regression Cost =============\n",
    "%  You should now implement the cost function for regularized linear \n",
    "%  regression. \n",
    "%\n",
    "function [J, grad] = linearRegCostFunction(X, y, theta, lambda)\n",
    "    %LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear\n",
    "    %regression with multiple variables\n",
    "    %   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the\n",
    "    %   cost of using theta as the parameter for linear regression to fit the\n",
    "    %   data points in X and y. Returns the cost in J and the gradient in grad\n",
    "\n",
    "    % Initialize some useful values\n",
    "    m = length(y); % number of training examples\n",
    "\n",
    "    % You need to return the following variables correctly\n",
    "    J = 0;\n",
    "    grad = zeros(size(theta));\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Compute the cost and gradient of regularized linear\n",
    "    %               regression for a particular choice of theta.\n",
    "    %\n",
    "    %               You should set J to the cost and grad to the gradient.\n",
    "    %\n",
    "    \n",
    "    error = X * theta - y;\n",
    "    reg = lambda * sum(theta(2:end,:) .^ 2) / (2 * m);\n",
    "    J = error' * error / (2 * m) + reg;\n",
    "    \n",
    "    grad(1,:) = X(:,1)' * error / m;\n",
    "    grad_reg = lambda * theta(2:end,:) / m;\n",
    "    grad(2:end,:) = X(:,2:end)' * error / m + grad_reg;\n",
    "\n",
    "    % =========================================================================\n",
    "\n",
    "    grad = grad(:);\n",
    "\n",
    "end\n",
    "\n",
    "theta = [1 ; 1];\n",
    "%J = linearRegCostFunction([ones(m, 1) X], y, theta, 1)\n",
    "%fprintf(['Cost at theta = [1 ; 1]: %f \\n(this value should be about 303.993192)\\n'], J);\n",
    "\n",
    "%% =========== Part 3: Regularized Linear Regression Gradient =============\n",
    "%  You should now implement the gradient for regularized linear \n",
    "%  regression.\n",
    "%\n",
    "\n",
    "[J, grad] = linearRegCostFunction([ones(m, 1) X], y, theta, 1)\n",
    "fprintf(['Gradient at theta = [1 ; 1]:  [%f; %f] \\n(this value should be about [-15.303016; 598.250744])\\n'], grad(1), grad(2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function [theta] = trainLinearReg(X, y, lambda)\n",
    "    %TRAINLINEARREG Trains linear regression given a dataset (X, y) and a\n",
    "    %regularization parameter lambda\n",
    "    %   [theta] = TRAINLINEARREG (X, y, lambda) trains linear regression using\n",
    "    %   the dataset (X, y) and regularization parameter lambda. Returns the\n",
    "    %   trained parameters theta.\n",
    "    %\n",
    "    \n",
    "    % Initialize Theta\n",
    "    initial_theta = zeros(size(X, 2), 1);\n",
    "    \n",
    "    % Create \"short hand\" for the cost function to be minimized\n",
    "    costFunction = @(t) linearRegCostFunction(X, y, t, lambda);\n",
    "    \n",
    "    % Now, costFunction is a function that takes in only one argument\n",
    "    options = optimset('MaxIter', 200, 'GradObj', 'on');\n",
    "    \n",
    "    % Minimize using fmincg\n",
    "    theta = fmincg(costFunction, initial_theta, options);\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     1 | Cost: 1.052435e+02\r",
      "Iteration     2 | Cost: 2.237391e+01\r",
      "Iteration     4 | Cost: 2.237391e+01\r",
      "Iteration     6 | Cost: 2.237391e+01\r",
      "Iteration     7 | Cost: 2.237391e+01\r",
      "Iteration     8 | Cost: 2.237391e+01\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 4: Train Linear Regression =============\n",
    "%  Once you have implemented the cost and gradient correctly, the\n",
    "%  trainLinearReg function will use your cost function to train \n",
    "%  regularized linear regression.\n",
    "% \n",
    "%  Write Up Note: The data is non-linear, so this will not give a great \n",
    "%                 fit.\n",
    "%\n",
    "\n",
    "%  Train linear regression with lambda = 0\n",
    "lambda = 0;\n",
    "[theta] = trainLinearReg([ones(m, 1) X], y, lambda);\n",
    "\n",
    "%  Plot fit over the data\n",
    "%plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);\n",
    "%xlabel('Change in water level (x)');\n",
    "%ylabel('Water flowing out of the dam (y)');\n",
    "%hold on;\n",
    "%plot(X, [ones(m, 1) X]*theta, '--', 'LineWidth', 2)\n",
    "%hold off;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 102 column 12\n",
      "    trainLinearReg at line 19 column 11\n",
      "    learningCurve at line 57 column 22\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "\n",
      "Iteration     3 | Cost: 9.860761e-32\n",
      "Iteration     2 | Cost: 3.286595e+00\n",
      "Iteration    28 | Cost: 2.842678e+00\n",
      "Iteration    24 | Cost: 1.315405e+01\n",
      "Iteration    27 | Cost: 1.944396e+01\n",
      "Iteration    13 | Cost: 2.009852e+01\n",
      "Iteration    13 | Cost: 1.817286e+01\n",
      "Iteration    13 | Cost: 2.260941e+01\n",
      "Iteration    38 | Cost: 2.326146e+01\n",
      "Iteration    12 | Cost: 2.431725e+01\n",
      "Iteration     4 | Cost: 2.237391e+01\n",
      "# Training Examples\tTrain Error\tCross Validation Error\n",
      "  \t1\t\t0.000000\t205.121096\n",
      "  \t2\t\t0.000000\t110.300366\n",
      "  \t3\t\t3.286595\t45.010231\n",
      "  \t4\t\t2.842678\t48.368911\n",
      "  \t5\t\t13.154049\t35.865165\n",
      "  \t6\t\t19.443963\t33.829962\n",
      "  \t7\t\t20.098522\t31.970986\n",
      "  \t8\t\t18.172859\t30.862446\n",
      "  \t9\t\t22.609405\t31.135998\n",
      "  \t10\t\t23.261462\t28.936207\n",
      "  \t11\t\t24.317250\t29.551432\n",
      "  \t12\t\t22.373906\t29.433818\n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 5: Learning Curve for Linear Regression =============\n",
    "%  Next, you should implement the learningCurve function. \n",
    "%\n",
    "%  Write Up Note: Since the model is underfitting the data, we expect to\n",
    "%                 see a graph with \"high bias\" -- slide 8 in ML-advice.pdf \n",
    "%\n",
    "function [error_train, error_val] = learningCurve(X, y, Xval, yval, lambda)\n",
    "    %LEARNINGCURVE Generates the train and cross validation set errors needed \n",
    "    %to plot a learning curve\n",
    "    %   [error_train, error_val] = ...\n",
    "    %       LEARNINGCURVE(X, y, Xval, yval, lambda) returns the train and\n",
    "    %       cross validation set errors for a learning curve. In particular, \n",
    "    %       it returns two vectors of the same length - error_train and \n",
    "    %       error_val. Then, error_train(i) contains the training error for\n",
    "    %       i examples (and similarly for error_val(i)).\n",
    "    %\n",
    "    %   In this function, you will compute the train and test errors for\n",
    "    %   dataset sizes from 1 up to m. In practice, when working with larger\n",
    "    %   datasets, you might want to do this in larger intervals.\n",
    "    %\n",
    "\n",
    "    % Number of training examples\n",
    "    m = size(X, 1);\n",
    "\n",
    "    % You need to return these values correctly\n",
    "    error_train = zeros(m, 1);\n",
    "    error_val   = zeros(m, 1);\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Fill in this function to return training errors in \n",
    "    %               error_train and the cross validation errors in error_val. \n",
    "    %               i.e., error_train(i) and \n",
    "    %               error_val(i) should give you the errors\n",
    "    %               obtained after training on i examples.\n",
    "    %\n",
    "    % Note: You should evaluate the training error on the first i training\n",
    "    %       examples (i.e., X(1:i, :) and y(1:i)).\n",
    "    %\n",
    "    %       For the cross-validation error, you should instead evaluate on\n",
    "    %       the _entire_ cross validation set (Xval and yval).\n",
    "    %\n",
    "    % Note: If you are using your cost function (linearRegCostFunction)\n",
    "    %       to compute the training and cross validation error, you should \n",
    "    %       call the function with the lambda argument set to 0. \n",
    "    %       Do note that you will still need to use lambda when running\n",
    "    %       the training to obtain the theta parameters.\n",
    "    %\n",
    "    % Hint: You can loop over the examples with the following:\n",
    "    %\n",
    "    %       for i = 1:m\n",
    "    %           % Compute train/cross validation errors using training examples \n",
    "    %           % X(1:i, :) and y(1:i), storing the result in \n",
    "    %           % error_train(i) and error_val(i)\n",
    "    %           ....\n",
    "    %           \n",
    "    %       end\n",
    "    %\n",
    "\n",
    "    % ---------------------- Sample Solution ----------------------\n",
    "    \n",
    "    %m = 1;\n",
    "    for i=1:m\n",
    "        [theta_train] = trainLinearReg(X(1:i,:), y(1:i,:), lambda);\n",
    "        err_tr = X(1:i,:) * theta_train - y(1:i,:);\n",
    "        error_train(i,:) = err_tr' * err_tr / (2 * i);\n",
    "        \n",
    "        err_val = Xval * theta_train - yval;\n",
    "        error_val(i,:) = err_val' * err_val / (2 * size(Xval,1));\n",
    "    end\n",
    "    \n",
    "    % -------------------------------------------------------------\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "end\n",
    "\n",
    "lambda = 0;\n",
    "[error_train, error_val] = learningCurve([ones(m, 1) X], y, [ones(size(Xval, 1), 1) Xval], yval, lambda);\n",
    "\n",
    "%[error_train, error_val] = learningCurve( [1 2; 1 3; 1 4; 1 5], [7;6;5;4], [1 7; 1 -2;], [2; 12], 7 )\n",
    "\n",
    "%plot(1:m, error_train, 1:m, error_val);\n",
    "%title('Learning curve for linear regression')\n",
    "%legend('Train', 'Cross Validation')\n",
    "%xlabel('Number of training examples')\n",
    "%ylabel('Error')\n",
    "%axis([0 13 0 150])\n",
    "\n",
    "fprintf('# Training Examples\\tTrain Error\\tCross Validation Error\\n');\n",
    "for i = 1:m\n",
    "    fprintf('  \\t%d\\t\\t%f\\t%f\\n', i, error_train(i), error_val(i));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%input:\n",
    "%[error_train, error_val] = learningCurve( [1 2; 1 3; 1 4; 1 5], [7;6;5;4], [1 7; 1 -2;], [2; 12], 7 )\n",
    "\n",
    "%output:\n",
    "%error_train =\n",
    "%   0.00000\n",
    "%   0.10889\n",
    "%   0.20165\n",
    "%   0.21267\n",
    "%error_val =\n",
    "%   12.5000\n",
    "%   11.1700\n",
    "%    8.3951\n",
    "%    5.4696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Training Example 1:\n",
      "  1.000000  \n",
      "  -0.362141  \n",
      "  -0.755087  \n",
      "  0.182226  \n",
      "  -0.706190  \n",
      "  0.306618  \n",
      "  -0.590878  \n",
      "  0.344516  \n",
      "  -0.508481  \n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 6: Feature Mapping for Polynomial Regression =============\n",
    "%  One solution to this is to use polynomial regression. You should now\n",
    "%  complete polyFeatures to map each example into its powers\n",
    "%\n",
    "function [X_poly] = polyFeatures(X, p)\n",
    "    %POLYFEATURES Maps X (1D vector) into the p-th power\n",
    "    %   [X_poly] = POLYFEATURES(X, p) takes a data matrix X (size m x 1) and\n",
    "    %   maps each example into its polynomial features where\n",
    "    %   X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^p];\n",
    "    %\n",
    "\n",
    "\n",
    "    % You need to return the following variables correctly.\n",
    "    X_poly = zeros(numel(X), p);\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Given a vector X, return a matrix X_poly where the p-th\n",
    "    %               column of X contains the values of X to the p-th power.\n",
    "    %\n",
    "    %\n",
    "    \n",
    "    %X\n",
    "\n",
    "    for i=1:size(X,1)\n",
    "        for j=1:p\n",
    "            X_poly(i, j) = X(i)^j;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    %X_poly\n",
    "\n",
    "    % =========================================================================\n",
    "end\n",
    "\n",
    "p = 8;\n",
    "\n",
    "% Map X onto Polynomial Features and Normalize\n",
    "X_poly = polyFeatures(X, p);\n",
    "[X_poly, mu, sigma] = featureNormalize(X_poly);  % Normalize\n",
    "X_poly = [ones(m, 1), X_poly];                   % Add Ones\n",
    "\n",
    "% Map X_poly_test and normalize (using mu and sigma)\n",
    "X_poly_test = polyFeatures(Xtest, p);\n",
    "X_poly_test = bsxfun(@minus, X_poly_test, mu);\n",
    "X_poly_test = bsxfun(@rdivide, X_poly_test, sigma);\n",
    "X_poly_test = [ones(size(X_poly_test, 1), 1), X_poly_test];         % Add Ones\n",
    "\n",
    "% Map X_poly_val and normalize (using mu and sigma)\n",
    "X_poly_val = polyFeatures(Xval, p);\n",
    "X_poly_val = bsxfun(@minus, X_poly_val, mu);\n",
    "X_poly_val = bsxfun(@rdivide, X_poly_val, sigma);\n",
    "X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];           % Add Ones\n",
    "\n",
    "fprintf('Normalized Training Example 1:\\n');\n",
    "fprintf('  %f  \\n', X_poly(1, :));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   200 | Cost: 1.031730e-01\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "    trainLinearReg at line 19 column 11\n",
      "    learningCurve at line 57 column 22\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "\n",
      "Iteration    25 | Cost: 1.643460e-31\n",
      "Iteration    11 | Cost: 3.779137e-29\n",
      "Iteration   200 | Cost: 1.254812e-12\n",
      "Iteration   200 | Cost: 3.256058e-04\n",
      "warning: division by zero\n",
      "Iteration   200 | Cost: 1.578230e-02\n",
      "Iteration   200 | Cost: 6.848065e-02\n",
      "Iteration   200 | Cost: 1.824513e-01\n",
      "Iteration   200 | Cost: 9.356609e-02\n",
      "Iteration   200 | Cost: 1.361207e-01\n",
      "Iteration   200 | Cost: 1.348129e-01\n",
      "Polynomial Regression (lambda = 0.000000)\n",
      "\n",
      "# Training Examples\tTrain Error\tCross Validation Error\n",
      "  \t1\t\t0.000000\t160.721900\n",
      "  \t2\t\t0.000000\t160.121510\n",
      "  \t3\t\t0.000000\t61.754825\n",
      "  \t4\t\t0.000000\t61.928895\n",
      "  \t5\t\t0.000000\t6.597879\n",
      "  \t6\t\t0.000326\t10.282412\n",
      "  \t7\t\t0.015782\t13.864886\n",
      "  \t8\t\t0.068481\t8.083144\n",
      "  \t9\t\t0.182451\t7.703350\n",
      "  \t10\t\t0.093566\t7.629622\n",
      "  \t11\t\t0.136121\t8.418546\n",
      "  \t12\t\t0.134813\t16.015622\n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 7: Learning Curve for Polynomial Regression =============\n",
    "%  Now, you will get to experiment with polynomial regression with multiple\n",
    "%  values of lambda. The code below runs polynomial regression with \n",
    "%  lambda = 0. You should try running the code with different values of\n",
    "%  lambda to see how the fit and learning curve change.\n",
    "%\n",
    "\n",
    "lambda = 0;\n",
    "[theta] = trainLinearReg(X_poly, y, lambda);\n",
    "\n",
    "% Plot training data and fit\n",
    "%figure(1);\n",
    "%plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);\n",
    "%plotFit(min(X), max(X), mu, sigma, theta, p);\n",
    "%xlabel('Change in water level (x)');\n",
    "%ylabel('Water flowing out of the dam (y)');\n",
    "%title (sprintf('Polynomial Regression Fit (lambda = %f)', lambda));\n",
    "%\n",
    "%figure(2);\n",
    "\n",
    "[error_train, error_val] = learningCurve(X_poly, y, X_poly_val, yval, lambda);\n",
    "    \n",
    "%plot(1:m, error_train, 1:m, error_val);\n",
    "%title(sprintf('Polynomial Regression Learning Curve (lambda = %f)', lambda));\n",
    "%xlabel('Number of training examples')\n",
    "%ylabel('Error')\n",
    "%axis([0 13 0 100])\n",
    "%legend('Train', 'Cross Validation')\n",
    "\n",
    "fprintf('Polynomial Regression (lambda = %f)\\n\\n', lambda);\n",
    "fprintf('# Training Examples\\tTrain Error\\tCross Validation Error\\n');\n",
    "for i = 1:m\n",
    "    fprintf('  \\t%d\\t\\t%f\\t%f\\n', i, error_train(i), error_val(i));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   200 | Cost: 1.318644e-01\n",
      "Iteration   200 | Cost: 1.866428e-01\n",
      "Iteration   200 | Cost: 2.550478e-01\n",
      "Iteration   200 | Cost: 3.850631e-01\n",
      "Iteration   200 | Cost: 6.692749e-01\n",
      "Iteration   200 | Cost: 1.443470e+00\n",
      "Iteration    99 | Cost: 3.101591e+00\n",
      "Iteration    67 | Cost: 7.268148e+00\n",
      "Iteration    39 | Cost: 1.586769e+01\n",
      "Iteration    24 | Cost: 3.337220e+01\n",
      "lambda\t\tTrain Error\tValidation Error\n",
      " 0.000000\t0.131864\t15.879999\n",
      " 0.001000\t0.151897\t17.066853\n",
      " 0.003000\t0.192957\t19.841554\n",
      " 0.010000\t0.221547\t16.954488\n",
      " 0.030000\t0.281853\t12.829588\n",
      " 0.100000\t0.459318\t7.587014\n",
      " 0.300000\t0.921760\t4.636833\n",
      " 1.000000\t2.076188\t4.260625\n",
      " 3.000000\t4.901351\t3.822907\n",
      " 10.000000\t16.092213\t9.945509\n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 8: Validation for Selecting Lambda =============\n",
    "%  You will now implement validationCurve to test various values of \n",
    "%  lambda on a validation set. You will then use this to select the\n",
    "%  \"best\" lambda value.\n",
    "%\n",
    "function [lambda_vec, error_train, error_val] = validationCurve(X, y, Xval, yval)\n",
    "    %VALIDATIONCURVE Generate the train and validation errors needed to\n",
    "    %plot a validation curve that we can use to select lambda\n",
    "    %   [lambda_vec, error_train, error_val] = ...\n",
    "    %       VALIDATIONCURVE(X, y, Xval, yval) returns the train\n",
    "    %       and validation errors (in error_train, error_val)\n",
    "    %       for different values of lambda. You are given the training set (X,\n",
    "    %       y) and validation set (Xval, yval).\n",
    "    %\n",
    "\n",
    "    % Selected values of lambda (you should not change this)\n",
    "    lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]';\n",
    "\n",
    "    % You need to return these variables correctly.\n",
    "    error_train = zeros(length(lambda_vec), 1);\n",
    "    error_val = zeros(length(lambda_vec), 1);\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Fill in this function to return training errors in \n",
    "    %               error_train and the validation errors in error_val. The \n",
    "    %               vector lambda_vec contains the different lambda parameters \n",
    "    %               to use for each calculation of the errors, i.e, \n",
    "    %               error_train(i), and error_val(i) should give \n",
    "    %               you the errors obtained after training with \n",
    "    %               lambda = lambda_vec(i)\n",
    "    %\n",
    "    % Note: You can loop over lambda_vec with the following:\n",
    "    %\n",
    "    %       for i = 1:length(lambda_vec)\n",
    "    %           lambda = lambda_vec(i);\n",
    "    %           % Compute train / val errors when training linear \n",
    "    %           % regression with regularization parameter lambda\n",
    "    %           % You should store the result in error_train(i)\n",
    "    %           % and error_val(i)\n",
    "    %           ....\n",
    "    %           \n",
    "    %       end\n",
    "    %\n",
    "    %\n",
    "    \n",
    "    m = size(X,1);\n",
    "    mval = size(Xval,1);\n",
    "\n",
    "    for i = 1:length(lambda_vec)\n",
    "        lambda = lambda_vec(i);\n",
    "        \n",
    "        [theta_train] = trainLinearReg(X, y, lambda);\n",
    "        err_tr = X * theta_train - y;\n",
    "        error_train(i,:) = err_tr' * err_tr / (2 * m);\n",
    "        \n",
    "        err_val = Xval * theta_train - yval;\n",
    "        error_val(i,:) = err_val' * err_val / (2 * mval);\n",
    "    end\n",
    "\n",
    "    % =========================================================================\n",
    "\n",
    "end\n",
    "\n",
    "[lambda_vec, error_train, error_val] = validationCurve(X_poly, y, X_poly_val, yval);\n",
    "\n",
    "%close all;\n",
    "%plot(lambda_vec, error_train, lambda_vec, error_val);\n",
    "%legend('Train', 'Cross Validation');\n",
    "%xlabel('lambda');\n",
    "%ylabel('Error');\n",
    "\n",
    "fprintf('lambda\\t\\tTrain Error\\tValidation Error\\n');\n",
    "for i = 1:length(lambda_vec)\n",
    "\tfprintf(' %f\\t%f\\t%f\\n', ...\n",
    "            lambda_vec(i), error_train(i), error_val(i));\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "0.16.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
