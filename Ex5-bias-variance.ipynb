{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: load: '/Users/jarmo/development/projects/stanford-machine-learning-course/exercises/ex5/ex5data1.mat' found by searching load path\r\n"
     ]
    }
   ],
   "source": [
    "%% Machine Learning Online Class\n",
    "%  Exercise 5 | Regularized Linear Regression and Bias-Variance\n",
    "%\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the\n",
    "%  exercise. You will need to complete the following functions:\n",
    "%\n",
    "%     linearRegCostFunction.m\n",
    "%     learningCurve.m\n",
    "%     validationCurve.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "%clear ; close all; clc\n",
    "\n",
    "%% =========== Part 1: Loading and Visualizing Data =============\n",
    "%  We start the exercise by first loading and visualizing the dataset. \n",
    "%  The following code will load the dataset into your environment and plot\n",
    "%  the data.\n",
    "%\n",
    "\n",
    "% Load Training Data\n",
    "%fprintf('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "% Load from ex5data1: \n",
    "% You will have X, y, Xval, yval, Xtest, ytest in your environment\n",
    "\n",
    "addpath (\"exercises/ex5\");\n",
    "load ('ex5data1.mat');\n",
    "\n",
    "% m = Number of examples\n",
    "m = size(X, 1);\n",
    "\n",
    "% Plot training data\n",
    "%plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);\n",
    "%xlabel('Change in water level (x)');\n",
    "%ylabel('Water flowing out of the dam (y)');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J =  303.99\n",
      "grad =\n",
      "\n",
      "   -15.303\n",
      "   598.251\n",
      "\n",
      "Gradient at theta = [1 ; 1]:  [-15.303016; 598.250744] \n",
      "(this value should be about [-15.303016; 598.250744])\n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 2: Regularized Linear Regression Cost =============\n",
    "%  You should now implement the cost function for regularized linear \n",
    "%  regression. \n",
    "%\n",
    "function [J, grad] = linearRegCostFunction(X, y, theta, lambda)\n",
    "    %LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear\n",
    "    %regression with multiple variables\n",
    "    %   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the\n",
    "    %   cost of using theta as the parameter for linear regression to fit the\n",
    "    %   data points in X and y. Returns the cost in J and the gradient in grad\n",
    "\n",
    "    % Initialize some useful values\n",
    "    m = length(y); % number of training examples\n",
    "\n",
    "    % You need to return the following variables correctly\n",
    "    J = 0;\n",
    "    grad = zeros(size(theta));\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Compute the cost and gradient of regularized linear\n",
    "    %               regression for a particular choice of theta.\n",
    "    %\n",
    "    %               You should set J to the cost and grad to the gradient.\n",
    "    %\n",
    "    \n",
    "    error = X * theta - y;\n",
    "    reg = lambda * sum(theta(2:end,:) .^ 2) / (2 * m);\n",
    "    J = error' * error / (2 * m) + reg;\n",
    "    \n",
    "    grad(1,:) = X(:,1)' * error / m;\n",
    "    grad_reg = lambda * theta(2:end,:) / m;\n",
    "    grad(2:end,:) = X(:,2:end)' * error / m + grad_reg;\n",
    "\n",
    "    % =========================================================================\n",
    "\n",
    "    grad = grad(:);\n",
    "\n",
    "end\n",
    "\n",
    "theta = [1 ; 1];\n",
    "%J = linearRegCostFunction([ones(m, 1) X], y, theta, 1)\n",
    "%fprintf(['Cost at theta = [1 ; 1]: %f \\n(this value should be about 303.993192)\\n'], J);\n",
    "\n",
    "%% =========== Part 3: Regularized Linear Regression Gradient =============\n",
    "%  You should now implement the gradient for regularized linear \n",
    "%  regression.\n",
    "%\n",
    "\n",
    "[J, grad] = linearRegCostFunction([ones(m, 1) X], y, theta, 1)\n",
    "fprintf(['Gradient at theta = [1 ; 1]:  [%f; %f] \\n(this value should be about [-15.303016; 598.250744])\\n'], grad(1), grad(2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function [theta] = trainLinearReg(X, y, lambda)\n",
    "    %TRAINLINEARREG Trains linear regression given a dataset (X, y) and a\n",
    "    %regularization parameter lambda\n",
    "    %   [theta] = TRAINLINEARREG (X, y, lambda) trains linear regression using\n",
    "    %   the dataset (X, y) and regularization parameter lambda. Returns the\n",
    "    %   trained parameters theta.\n",
    "    %\n",
    "    \n",
    "    % Initialize Theta\n",
    "    initial_theta = zeros(size(X, 2), 1);\n",
    "    \n",
    "    % Create \"short hand\" for the cost function to be minimized\n",
    "    costFunction = @(t) linearRegCostFunction(X, y, t, lambda);\n",
    "    \n",
    "    % Now, costFunction is a function that takes in only one argument\n",
    "    options = optimset('MaxIter', 200, 'GradObj', 'on');\n",
    "    \n",
    "    % Minimize using fmincg\n",
    "    theta = fmincg(costFunction, initial_theta, options);\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     1 | Cost: 1.052435e+02\r",
      "Iteration     2 | Cost: 2.237391e+01\r",
      "Iteration     4 | Cost: 2.237391e+01\r",
      "Iteration     6 | Cost: 2.237391e+01\r",
      "Iteration     7 | Cost: 2.237391e+01\r",
      "Iteration     8 | Cost: 2.237391e+01\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 4: Train Linear Regression =============\n",
    "%  Once you have implemented the cost and gradient correctly, the\n",
    "%  trainLinearReg function will use your cost function to train \n",
    "%  regularized linear regression.\n",
    "% \n",
    "%  Write Up Note: The data is non-linear, so this will not give a great \n",
    "%                 fit.\n",
    "%\n",
    "\n",
    "%  Train linear regression with lambda = 0\n",
    "lambda = 0;\n",
    "[theta] = trainLinearReg([ones(m, 1) X], y, lambda);\n",
    "\n",
    "%  Plot fit over the data\n",
    "%plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);\n",
    "%xlabel('Change in water level (x)');\n",
    "%ylabel('Water flowing out of the dam (y)');\n",
    "%hold on;\n",
    "%plot(X, [ones(m, 1) X]*theta, '--', 'LineWidth', 2)\n",
    "%hold off;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 102 column 12\n",
      "    trainLinearReg at line 19 column 11\n",
      "    learningCurve at line 57 column 22\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "warning: division by zero\n",
      "\n",
      "Iteration     3 | Cost: 9.860761e-32\n",
      "Iteration     2 | Cost: 3.286595e+00\n",
      "Iteration    28 | Cost: 2.842678e+00\n",
      "Iteration    24 | Cost: 1.315405e+01\n",
      "Iteration    27 | Cost: 1.944396e+01\n",
      "Iteration    13 | Cost: 2.009852e+01\n",
      "Iteration    13 | Cost: 1.817286e+01\n",
      "Iteration    13 | Cost: 2.260941e+01\n",
      "Iteration    38 | Cost: 2.326146e+01\n",
      "Iteration    12 | Cost: 2.431725e+01\n",
      "Iteration     4 | Cost: 2.237391e+01\n",
      "# Training Examples\tTrain Error\tCross Validation Error\n",
      "  \t1\t\t0.000000\t205.121096\n",
      "  \t2\t\t0.000000\t110.300366\n",
      "  \t3\t\t3.286595\t45.010231\n",
      "  \t4\t\t2.842678\t48.368911\n",
      "  \t5\t\t13.154049\t35.865165\n",
      "  \t6\t\t19.443963\t33.829962\n",
      "  \t7\t\t20.098522\t31.970986\n",
      "  \t8\t\t18.172859\t30.862446\n",
      "  \t9\t\t22.609405\t31.135998\n",
      "  \t10\t\t23.261462\t28.936207\n",
      "  \t11\t\t24.317250\t29.551432\n",
      "  \t12\t\t22.373906\t29.433818\n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 5: Learning Curve for Linear Regression =============\n",
    "%  Next, you should implement the learningCurve function. \n",
    "%\n",
    "%  Write Up Note: Since the model is underfitting the data, we expect to\n",
    "%                 see a graph with \"high bias\" -- slide 8 in ML-advice.pdf \n",
    "%\n",
    "function [error_train, error_val] = learningCurve(X, y, Xval, yval, lambda)\n",
    "    %LEARNINGCURVE Generates the train and cross validation set errors needed \n",
    "    %to plot a learning curve\n",
    "    %   [error_train, error_val] = ...\n",
    "    %       LEARNINGCURVE(X, y, Xval, yval, lambda) returns the train and\n",
    "    %       cross validation set errors for a learning curve. In particular, \n",
    "    %       it returns two vectors of the same length - error_train and \n",
    "    %       error_val. Then, error_train(i) contains the training error for\n",
    "    %       i examples (and similarly for error_val(i)).\n",
    "    %\n",
    "    %   In this function, you will compute the train and test errors for\n",
    "    %   dataset sizes from 1 up to m. In practice, when working with larger\n",
    "    %   datasets, you might want to do this in larger intervals.\n",
    "    %\n",
    "\n",
    "    % Number of training examples\n",
    "    m = size(X, 1);\n",
    "\n",
    "    % You need to return these values correctly\n",
    "    error_train = zeros(m, 1);\n",
    "    error_val   = zeros(m, 1);\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Fill in this function to return training errors in \n",
    "    %               error_train and the cross validation errors in error_val. \n",
    "    %               i.e., error_train(i) and \n",
    "    %               error_val(i) should give you the errors\n",
    "    %               obtained after training on i examples.\n",
    "    %\n",
    "    % Note: You should evaluate the training error on the first i training\n",
    "    %       examples (i.e., X(1:i, :) and y(1:i)).\n",
    "    %\n",
    "    %       For the cross-validation error, you should instead evaluate on\n",
    "    %       the _entire_ cross validation set (Xval and yval).\n",
    "    %\n",
    "    % Note: If you are using your cost function (linearRegCostFunction)\n",
    "    %       to compute the training and cross validation error, you should \n",
    "    %       call the function with the lambda argument set to 0. \n",
    "    %       Do note that you will still need to use lambda when running\n",
    "    %       the training to obtain the theta parameters.\n",
    "    %\n",
    "    % Hint: You can loop over the examples with the following:\n",
    "    %\n",
    "    %       for i = 1:m\n",
    "    %           % Compute train/cross validation errors using training examples \n",
    "    %           % X(1:i, :) and y(1:i), storing the result in \n",
    "    %           % error_train(i) and error_val(i)\n",
    "    %           ....\n",
    "    %           \n",
    "    %       end\n",
    "    %\n",
    "\n",
    "    % ---------------------- Sample Solution ----------------------\n",
    "    \n",
    "    %m = 1;\n",
    "    for i=1:m\n",
    "        [theta_train] = trainLinearReg(X(1:i,:), y(1:i,:), lambda);\n",
    "        err_tr = X(1:i,:) * theta_train - y(1:i,:);\n",
    "        error_train(i,:) = err_tr' * err_tr / (2 * i);\n",
    "        \n",
    "        err_val = Xval * theta_train - yval;\n",
    "        error_val(i,:) = err_val' * err_val / (2 * size(Xval,1));\n",
    "    end\n",
    "    \n",
    "    % -------------------------------------------------------------\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "end\n",
    "\n",
    "lambda = 0;\n",
    "[error_train, error_val] = learningCurve([ones(m, 1) X], y, [ones(size(Xval, 1), 1) Xval], yval, lambda);\n",
    "\n",
    "%[error_train, error_val] = learningCurve( [1 2; 1 3; 1 4; 1 5], [7;6;5;4], [1 7; 1 -2;], [2; 12], 7 )\n",
    "\n",
    "%plot(1:m, error_train, 1:m, error_val);\n",
    "%title('Learning curve for linear regression')\n",
    "%legend('Train', 'Cross Validation')\n",
    "%xlabel('Number of training examples')\n",
    "%ylabel('Error')\n",
    "%axis([0 13 0 150])\n",
    "\n",
    "fprintf('# Training Examples\\tTrain Error\\tCross Validation Error\\n');\n",
    "for i = 1:m\n",
    "    fprintf('  \\t%d\\t\\t%f\\t%f\\n', i, error_train(i), error_val(i));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%input:\n",
    "%[error_train, error_val] = learningCurve( [1 2; 1 3; 1 4; 1 5], [7;6;5;4], [1 7; 1 -2;], [2; 12], 7 )\n",
    "\n",
    "%output:\n",
    "%error_train =\n",
    "%   0.00000\n",
    "%   0.10889\n",
    "%   0.20165\n",
    "%   0.21267\n",
    "%error_val =\n",
    "%   12.5000\n",
    "%   11.1700\n",
    "%    8.3951\n",
    "%    5.4696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =\n",
      "\n",
      "  -15.9368\n",
      "  -29.1530\n",
      "   36.1895\n",
      "   37.4922\n",
      "  -48.0588\n",
      "   -8.9415\n",
      "   15.3078\n",
      "  -34.7063\n",
      "    1.3892\n",
      "  -44.3838\n",
      "    7.0135\n",
      "   22.7627\n",
      "\n",
      "X_poly =\n",
      "\n",
      " Columns 1 through 6:\n",
      "\n",
      "  -1.5937e+01   2.5398e+02  -4.0476e+03   6.4506e+04  -1.0280e+06   1.6383e+07\n",
      "  -2.9153e+01   8.4990e+02  -2.4777e+04   7.2232e+05  -2.1058e+07   6.1390e+08\n",
      "   3.6190e+01   1.3097e+03   4.7397e+04   1.7153e+06   6.2075e+07   2.2465e+09\n",
      "   3.7492e+01   1.4057e+03   5.2701e+04   1.9759e+06   7.4080e+07   2.7774e+09\n",
      "  -4.8059e+01   2.3097e+03  -1.1100e+05   5.3345e+06  -2.5637e+08   1.2321e+10\n",
      "  -8.9415e+00   7.9950e+01  -7.1487e+02   6.3919e+03  -5.7153e+04   5.1103e+05\n",
      "   1.5308e+01   2.3433e+02   3.5871e+03   5.4910e+04   8.4055e+05   1.2867e+07\n",
      "  -3.4706e+01   1.2045e+03  -4.1805e+04   1.4509e+06  -5.0355e+07   1.7476e+09\n",
      "   1.3892e+00   1.9297e+00   2.6807e+00   3.7239e+00   5.1731e+00   7.1863e+00\n",
      "  -4.4384e+01   1.9699e+03  -8.7432e+04   3.8806e+06  -1.7223e+08   7.6444e+09\n",
      "   7.0135e+00   4.9189e+01   3.4499e+02   2.4196e+03   1.6970e+04   1.1902e+05\n",
      "   2.2763e+01   5.1814e+02   1.1794e+04   2.6847e+05   6.1112e+06   1.3911e+08\n",
      "\n",
      " Columns 7 and 8:\n",
      "\n",
      "  -2.6110e+08   4.1610e+09\n",
      "  -1.7897e+10   5.2175e+11\n",
      "   8.1298e+10   2.9422e+12\n",
      "   1.0413e+11   3.9041e+12\n",
      "  -5.9212e+11   2.8457e+13\n",
      "  -4.5694e+06   4.0857e+07\n",
      "   1.9696e+08   3.0151e+09\n",
      "  -6.0653e+10   2.1051e+12\n",
      "   9.9828e+00   1.3868e+01\n",
      "  -3.3929e+11   1.5059e+13\n",
      "   8.3473e+05   5.8544e+06\n",
      "   3.1665e+09   7.2077e+10\n",
      "\n",
      "X =\n",
      "\n",
      "  -33.3180\n",
      "  -37.9122\n",
      "  -51.2069\n",
      "   -6.1326\n",
      "   21.2612\n",
      "  -40.3195\n",
      "  -14.5415\n",
      "   32.5598\n",
      "   13.3934\n",
      "   44.2099\n",
      "   -1.1427\n",
      "  -12.7669\n",
      "   34.0545\n",
      "   39.2235\n",
      "    1.9745\n",
      "   29.6218\n",
      "  -23.6696\n",
      "   -9.0118\n",
      "  -55.9406\n",
      "  -35.7086\n",
      "    9.5102\n",
      "\n",
      "X_poly =\n",
      "\n",
      " Columns 1 through 6:\n",
      "\n",
      "  -3.3318e+01   1.1101e+03  -3.6986e+04   1.2323e+06  -4.1058e+07   1.3680e+09\n",
      "  -3.7912e+01   1.4373e+03  -5.4492e+04   2.0659e+06  -7.8324e+07   2.9694e+09\n",
      "  -5.1207e+01   2.6222e+03  -1.3427e+05   6.8757e+06  -3.5208e+08   1.8029e+10\n",
      "  -6.1326e+00   3.7609e+01  -2.3064e+02   1.4144e+03  -8.6740e+03   5.3194e+04\n",
      "   2.1261e+01   4.5204e+02   9.6109e+03   2.0434e+05   4.3445e+06   9.2369e+07\n",
      "  -4.0320e+01   1.6257e+03  -6.5546e+04   2.6428e+06  -1.0656e+08   4.2963e+09\n",
      "  -1.4542e+01   2.1146e+02  -3.0749e+03   4.4714e+04  -6.5021e+05   9.4550e+06\n",
      "   3.2560e+01   1.0601e+03   3.4518e+04   1.1239e+06   3.6594e+07   1.1915e+09\n",
      "   1.3393e+01   1.7938e+02   2.4026e+03   3.2179e+04   4.3098e+05   5.7723e+06\n",
      "   4.4210e+01   1.9545e+03   8.6409e+04   3.8201e+06   1.6889e+08   7.4665e+09\n",
      "  -1.1427e+00   1.3057e+00  -1.4920e+00   1.7049e+00  -1.9481e+00   2.2261e+00\n",
      "  -1.2767e+01   1.6299e+02  -2.0809e+03   2.6567e+04  -3.3917e+05   4.3302e+06\n",
      "   3.4055e+01   1.1597e+03   3.9493e+04   1.3449e+06   4.5801e+07   1.5597e+09\n",
      "   3.9224e+01   1.5385e+03   6.0345e+04   2.3669e+06   9.2839e+07   3.6415e+09\n",
      "   1.9745e+00   3.8986e+00   7.6978e+00   1.5199e+01   3.0011e+01   5.9257e+01\n",
      "   2.9622e+01   8.7745e+02   2.5992e+04   7.6992e+05   2.2806e+07   6.7556e+08\n",
      "  -2.3670e+01   5.6025e+02  -1.3261e+04   3.1388e+05  -7.4295e+06   1.7585e+08\n",
      "  -9.0118e+00   8.1213e+01  -7.3187e+02   6.5955e+03  -5.9437e+04   5.3564e+05\n",
      "  -5.5941e+01   3.1293e+03  -1.7506e+05   9.7928e+06  -5.4782e+08   3.0645e+10\n",
      "  -3.5709e+01   1.2751e+03  -4.5532e+04   1.6259e+06  -5.8058e+07   2.0732e+09\n",
      "   9.5102e+00   9.0444e+01   8.6014e+02   8.1801e+03   7.7795e+04   7.3984e+05\n",
      "\n",
      " Columns 7 and 8:\n",
      "\n",
      "  -4.5578e+10   1.5186e+12\n",
      "  -1.1258e+11   4.2680e+12\n",
      "  -9.2321e+11   4.7275e+13\n",
      "  -3.2622e+05   2.0006e+06\n",
      "   1.9639e+09   4.1754e+10\n",
      "  -1.7322e+11   6.9843e+12\n",
      "  -1.3749e+08   1.9993e+09\n",
      "   3.8794e+10   1.2631e+12\n",
      "   7.7311e+07   1.0355e+09\n",
      "   3.3009e+11   1.4593e+13\n",
      "  -2.5437e+00   2.9066e+00\n",
      "  -5.5283e+07   7.0579e+08\n",
      "   5.3116e+10   1.8088e+12\n",
      "   1.4283e+11   5.6024e+12\n",
      "   1.1700e+02   2.3102e+02\n",
      "   2.0011e+10   5.9277e+11\n",
      "  -4.1624e+09   9.8522e+10\n",
      "  -4.8270e+06   4.3500e+07\n",
      "  -1.7143e+12   9.5899e+13\n",
      "  -7.4030e+10   2.6435e+12\n",
      "   7.0361e+06   6.6914e+07\n",
      "\n",
      "X =\n",
      "\n",
      "  -16.7465\n",
      "  -14.5775\n",
      "   34.5158\n",
      "  -47.0101\n",
      "   36.9751\n",
      "  -40.6861\n",
      "   -4.4720\n",
      "   26.5336\n",
      "  -42.7977\n",
      "   25.3741\n",
      "  -31.1096\n",
      "   27.3118\n",
      "   -3.2639\n",
      "   -1.8183\n",
      "  -40.7197\n",
      "  -50.0132\n",
      "  -17.4118\n",
      "    3.5882\n",
      "    7.0855\n",
      "   46.2824\n",
      "   14.6123\n",
      "\n",
      "X_poly =\n",
      "\n",
      " Columns 1 through 6:\n",
      "\n",
      "  -1.6747e+01   2.8045e+02  -4.6965e+03   7.8650e+04  -1.3171e+06   2.2057e+07\n",
      "  -1.4577e+01   2.1250e+02  -3.0978e+03   4.5157e+04  -6.5828e+05   9.5961e+06\n",
      "   3.4516e+01   1.1913e+03   4.1120e+04   1.4193e+06   4.8988e+07   1.6908e+09\n",
      "  -4.7010e+01   2.2099e+03  -1.0389e+05   4.8839e+06  -2.2959e+08   1.0793e+10\n",
      "   3.6975e+01   1.3672e+03   5.0551e+04   1.8691e+06   6.9111e+07   2.5554e+09\n",
      "  -4.0686e+01   1.6554e+03  -6.7350e+04   2.7402e+06  -1.1149e+08   4.5360e+09\n",
      "  -4.4720e+00   1.9999e+01  -8.9435e+01   3.9996e+02  -1.7886e+03   7.9987e+03\n",
      "   2.6534e+01   7.0403e+02   1.8681e+04   4.9566e+05   1.3152e+07   3.4896e+08\n",
      "  -4.2798e+01   1.8316e+03  -7.8390e+04   3.3549e+06  -1.4358e+08   6.1450e+09\n",
      "   2.5374e+01   6.4384e+02   1.6337e+04   4.1454e+05   1.0518e+07   2.6690e+08\n",
      "  -3.1110e+01   9.6780e+02  -3.0108e+04   9.3665e+05  -2.9139e+07   9.0649e+08\n",
      "   2.7312e+01   7.4593e+02   2.0373e+04   5.5642e+05   1.5197e+07   4.1505e+08\n",
      "  -3.2639e+00   1.0653e+01  -3.4769e+01   1.1348e+02  -3.7039e+02   1.2089e+03\n",
      "  -1.8183e+00   3.3061e+00  -6.0115e+00   1.0930e+01  -1.9875e+01   3.6138e+01\n",
      "  -4.0720e+01   1.6581e+03  -6.7517e+04   2.7493e+06  -1.1195e+08   4.5585e+09\n",
      "  -5.0013e+01   2.5013e+03  -1.2510e+05   6.2566e+06  -3.1291e+08   1.5650e+10\n",
      "  -1.7412e+01   3.0317e+02  -5.2787e+03   9.1912e+04  -1.6003e+06   2.7865e+07\n",
      "   3.5882e+00   1.2875e+01   4.6198e+01   1.6577e+02   5.9481e+02   2.1343e+03\n",
      "   7.0855e+00   5.0204e+01   3.5572e+02   2.5204e+03   1.7859e+04   1.2654e+05\n",
      "   4.6282e+01   2.1421e+03   9.9140e+04   4.5884e+06   2.1236e+08   9.8286e+09\n",
      "   1.4612e+01   2.1352e+02   3.1200e+03   4.5590e+04   6.6618e+05   9.7344e+06\n",
      "\n",
      " Columns 7 and 8:\n",
      "\n",
      "  -3.6938e+08   6.1859e+09\n",
      "  -1.3989e+08   2.0392e+09\n",
      "   5.8361e+10   2.0144e+12\n",
      "  -5.0738e+11   2.3852e+13\n",
      "   9.4486e+10   3.4936e+12\n",
      "  -1.8455e+11   7.5088e+12\n",
      "  -3.5770e+04   1.5996e+05\n",
      "   9.2593e+09   2.4568e+11\n",
      "  -2.6299e+11   1.1255e+13\n",
      "   6.7723e+09   1.7184e+11\n",
      "  -2.8200e+10   8.7730e+11\n",
      "   1.1336e+10   3.0960e+11\n",
      "  -3.9457e+03   1.2878e+04\n",
      "  -6.5708e+01   1.1948e+02\n",
      "  -1.8562e+11   7.5585e+12\n",
      "  -7.8270e+11   3.9145e+13\n",
      "  -4.8518e+08   8.4478e+09\n",
      "   7.6583e+03   2.7479e+04\n",
      "   8.9657e+05   6.3526e+06\n",
      "   4.5489e+11   2.1054e+13\n",
      "   1.4224e+08   2.0785e+09\n",
      "\n",
      "Normalized Training Example 1:\n",
      "  1.000000  \n",
      "  -0.362141  \n",
      "  -0.755087  \n",
      "  0.182226  \n",
      "  -0.706190  \n",
      "  0.306618  \n",
      "  -0.590878  \n",
      "  0.344516  \n",
      "  -0.508481  \n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 6: Feature Mapping for Polynomial Regression =============\n",
    "%  One solution to this is to use polynomial regression. You should now\n",
    "%  complete polyFeatures to map each example into its powers\n",
    "%\n",
    "function [X_poly] = polyFeatures(X, p)\n",
    "    %POLYFEATURES Maps X (1D vector) into the p-th power\n",
    "    %   [X_poly] = POLYFEATURES(X, p) takes a data matrix X (size m x 1) and\n",
    "    %   maps each example into its polynomial features where\n",
    "    %   X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^p];\n",
    "    %\n",
    "\n",
    "\n",
    "    % You need to return the following variables correctly.\n",
    "    X_poly = zeros(numel(X), p);\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Given a vector X, return a matrix X_poly where the p-th\n",
    "    %               column of X contains the values of X to the p-th power.\n",
    "    %\n",
    "    %\n",
    "    \n",
    "    X\n",
    "\n",
    "    for i=1:size(X,1)\n",
    "        for j=1:p\n",
    "            X_poly(i, j) = X(i)^j;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    X_poly\n",
    "\n",
    "    % =========================================================================\n",
    "end\n",
    "\n",
    "p = 8;\n",
    "\n",
    "% Map X onto Polynomial Features and Normalize\n",
    "X_poly = polyFeatures(X, p);\n",
    "[X_poly, mu, sigma] = featureNormalize(X_poly);  % Normalize\n",
    "X_poly = [ones(m, 1), X_poly];                   % Add Ones\n",
    "\n",
    "% Map X_poly_test and normalize (using mu and sigma)\n",
    "X_poly_test = polyFeatures(Xtest, p);\n",
    "X_poly_test = bsxfun(@minus, X_poly_test, mu);\n",
    "X_poly_test = bsxfun(@rdivide, X_poly_test, sigma);\n",
    "X_poly_test = [ones(size(X_poly_test, 1), 1), X_poly_test];         % Add Ones\n",
    "\n",
    "% Map X_poly_val and normalize (using mu and sigma)\n",
    "X_poly_val = polyFeatures(Xval, p);\n",
    "X_poly_val = bsxfun(@minus, X_poly_val, mu);\n",
    "X_poly_val = bsxfun(@rdivide, X_poly_val, sigma);\n",
    "X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];           % Add Ones\n",
    "\n",
    "fprintf('Normalized Training Example 1:\\n');\n",
    "fprintf('  %f  \\n', X_poly(1, :));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% =========== Part 7: Learning Curve for Polynomial Regression =============\n",
    "%  Now, you will get to experiment with polynomial regression with multiple\n",
    "%  values of lambda. The code below runs polynomial regression with \n",
    "%  lambda = 0. You should try running the code with different values of\n",
    "%  lambda to see how the fit and learning curve change.\n",
    "%\n",
    "\n",
    "lambda = 0;\n",
    "[theta] = trainLinearReg(X_poly, y, lambda);\n",
    "\n",
    "% Plot training data and fit\n",
    "%figure(1);\n",
    "%plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);\n",
    "%plotFit(min(X), max(X), mu, sigma, theta, p);\n",
    "%xlabel('Change in water level (x)');\n",
    "%ylabel('Water flowing out of the dam (y)');\n",
    "%title (sprintf('Polynomial Regression Fit (lambda = %f)', lambda));\n",
    "%\n",
    "%figure(2);\n",
    "\n",
    "[error_train, error_val] = learningCurve(X_poly, y, X_poly_val, yval, lambda);\n",
    "    \n",
    "%plot(1:m, error_train, 1:m, error_val);\n",
    "%title(sprintf('Polynomial Regression Learning Curve (lambda = %f)', lambda));\n",
    "%xlabel('Number of training examples')\n",
    "%ylabel('Error')\n",
    "%axis([0 13 0 100])\n",
    "%legend('Train', 'Cross Validation')\n",
    "\n",
    "fprintf('Polynomial Regression (lambda = %f)\\n\\n', lambda);\n",
    "fprintf('# Training Examples\\tTrain Error\\tCross Validation Error\\n');\n",
    "for i = 1:m\n",
    "    fprintf('  \\t%d\\t\\t%f\\t%f\\n', i, error_train(i), error_val(i));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% =========== Part 8: Validation for Selecting Lambda =============\n",
    "%  You will now implement validationCurve to test various values of \n",
    "%  lambda on a validation set. You will then use this to select the\n",
    "%  \"best\" lambda value.\n",
    "%\n",
    "\n",
    "[lambda_vec, error_train, error_val] = validationCurve(X_poly, y, X_poly_val, yval);\n",
    "\n",
    "%close all;\n",
    "%plot(lambda_vec, error_train, lambda_vec, error_val);\n",
    "%legend('Train', 'Cross Validation');\n",
    "%xlabel('lambda');\n",
    "%ylabel('Error');\n",
    "\n",
    "fprintf('lambda\\t\\tTrain Error\\tValidation Error\\n');\n",
    "for i = 1:length(lambda_vec)\n",
    "\tfprintf(' %f\\t%f\\t%f\\n', ...\n",
    "            lambda_vec(i), error_train(i), error_val(i));\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "0.16.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
