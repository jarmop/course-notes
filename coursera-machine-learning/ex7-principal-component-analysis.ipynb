{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: load: '/Users/jarmo/development/projects/stanford-machine-learning-course/exercises/ex7/ex7data1.mat' found by searching load path\r\n"
     ]
    }
   ],
   "source": [
    "%% Machine Learning Online Class\n",
    "%  Exercise 7 | Principle Component Analysis and K-Means Clustering\n",
    "%\n",
    "%  Instructions\n",
    "%  ------------\n",
    "%\n",
    "%  This file contains code that helps you get started on the\n",
    "%  exercise. You will need to complete the following functions:\n",
    "%\n",
    "%     pca.m\n",
    "%     projectData.m\n",
    "%     recoverData.m\n",
    "%     computeCentroids.m\n",
    "%     findClosestCentroids.m\n",
    "%     kMeansInitCentroids.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "%clear ; close all; clc\n",
    "\n",
    "%% ================== Part 1: Load Example Dataset  ===================\n",
    "%  We start this exercise by using a small dataset that is easily to\n",
    "%  visualize\n",
    "%\n",
    "%fprintf('Visualizing example dataset for PCA.\\n\\n');\n",
    "\n",
    "%  The following command loads the dataset. You should now have the \n",
    "%  variable X in your environment\n",
    "addpath('exercises/ex7');\n",
    "load ('ex7data1.mat');\n",
    "\n",
    "%  Visualize the example dataset\n",
    "%plot(X(:, 1), X(:, 2), 'bo');\n",
    "%axis([0.5 6.5 2 8]); axis square;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% =============== Part 2: Principal Component Analysis ===============\n",
    "%  You should now implement PCA, a dimension reduction technique. You\n",
    "%  should complete the code in pca.m\n",
    "%\n",
    "%fprintf('\\nRunning PCA on example dataset.\\n\\n');\n",
    "\n",
    "%  Before running PCA, it is important to first normalize X\n",
    "[X_norm, mu, sigma] = featureNormalize(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top eigenvector: \n",
      " U(:,1) = -0.707107 -0.707107 \n",
      "\n",
      "(you should expect to see -0.707107 -0.707107)\n"
     ]
    }
   ],
   "source": [
    "function [U, S] = pca(X)\n",
    "    %PCA Run principal component analysis on the dataset X\n",
    "    %   [U, S, X] = pca(X) computes eigenvectors of the covariance matrix of X\n",
    "    %   Returns the eigenvectors U, the eigenvalues (on diagonal) in S\n",
    "    %\n",
    "\n",
    "    % Useful values\n",
    "    [m, n] = size(X);\n",
    "\n",
    "    % You need to return the following variables correctly.\n",
    "    U = zeros(n);\n",
    "    S = zeros(n);\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: You should first compute the covariance matrix. Then, you\n",
    "    %               should use the \"svd\" function to compute the eigenvectors\n",
    "    %               and eigenvalues of the covariance matrix.\n",
    "    %\n",
    "    % Note: When computing the covariance matrix, remember to divide by m (the\n",
    "    %       number of examples).\n",
    "    %\n",
    "    \n",
    "    Sigma = X'*X / m;\n",
    "\n",
    "    [U, S, V] = svd(Sigma);\n",
    "\n",
    "    % =========================================================================\n",
    "\n",
    "end\n",
    "\n",
    "%  Run PCA\n",
    "[U, S] = pca(X_norm);\n",
    "\n",
    "%  Compute mu, the mean of the each feature\n",
    "\n",
    "%  Draw the eigenvectors centered at mean of data. These lines show the\n",
    "%  directions of maximum variations in the dataset.\n",
    "%hold on;\n",
    "%drawLine(mu, mu + 1.5 * S(1,1) * U(:,1)', '-k', 'LineWidth', 2);\n",
    "%drawLine(mu, mu + 1.5 * S(2,2) * U(:,2)', '-k', 'LineWidth', 2);\n",
    "%hold off;\n",
    "\n",
    "fprintf('Top eigenvector: \\n');\n",
    "fprintf(' U(:,1) = %f %f \\n', U(1,1), U(2,1));\n",
    "fprintf('\\n(you should expect to see -0.707107 -0.707107)\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection of the first example: 1.481274\n",
      "\n",
      "(this value should be about 1.481274)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% =================== Part 3: Dimension Reduction ===================\n",
    "%  You should now implement the projection step to map the data onto the \n",
    "%  first k eigenvectors. The code will then plot the data in this reduced \n",
    "%  dimensional space.  This will show you what the data looks like when \n",
    "%  using only the corresponding eigenvectors to reconstruct it.\n",
    "%\n",
    "%  You should complete the code in projectData.m\n",
    "%\n",
    "%fprintf('\\nDimension reduction on example dataset.\\n\\n');\n",
    "\n",
    "%  Plot the normalized dataset (returned from pca)\n",
    "%plot(X_norm(:, 1), X_norm(:, 2), 'bo');\n",
    "%axis([-4 3 -4 3]); axis square\n",
    "\n",
    "function Z = projectData(X, U, K)\n",
    "    %PROJECTDATA Computes the reduced data representation when projecting only\n",
    "    %on to the top k eigenvectors\n",
    "    %   Z = projectData(X, U, K) computes the projection of\n",
    "    %   the normalized inputs X into the reduced dimensional space spanned by\n",
    "    %   the first K columns of U. It returns the projected examples in Z.\n",
    "    %\n",
    "\n",
    "    % You need to return the following variables correctly.\n",
    "    Z = zeros(size(X, 1), K);\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Compute the projection of the data using only the top K\n",
    "    %               eigenvectors in U (first K columns).\n",
    "    %               For the i-th example X(i,:), the projection on to the k-th\n",
    "    %               eigenvector is given as follows:\n",
    "    %                    x = X(i, :)';\n",
    "    %                    projection_k = x' * U(:, k);\n",
    "    %\n",
    "    \n",
    "    Z = X * U(:, 1:K);\n",
    "\n",
    "    % =============================================================\n",
    "\n",
    "end\n",
    "\n",
    "%  Project the data onto K = 1 dimension\n",
    "K = 1;\n",
    "Z = projectData(X_norm, U, K);\n",
    "fprintf('Projection of the first example: %f\\n', Z(1));\n",
    "fprintf('\\n(this value should be about 1.481274)\\n\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of the first example: -1.047419 -1.047419\n",
      "\n",
      "(this value should be about  -1.047419 -1.047419)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function X_rec = recoverData(Z, U, K)\n",
    "    %RECOVERDATA Recovers an approximation of the original data when using the\n",
    "    %projected data\n",
    "    %   X_rec = RECOVERDATA(Z, U, K) recovers an approximation the\n",
    "    %   original data that has been reduced to K dimensions. It returns the\n",
    "    %   approximate reconstruction in X_rec.\n",
    "    %\n",
    "\n",
    "    % You need to return the following variables correctly.\n",
    "    X_rec = zeros(size(Z, 1), size(U, 1));\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Compute the approximation of the data by projecting back\n",
    "    %               onto the original space using the top K eigenvectors in U.\n",
    "    %\n",
    "    %               For the i-th example Z(i,:), the (approximate)\n",
    "    %               recovered data for dimension j is given as follows:\n",
    "    %                    v = Z(i, :)';\n",
    "    %                    recovered_j = v' * U(j, 1:K)';\n",
    "    %\n",
    "    %               Notice that U(j, 1:K) is a row vector.\n",
    "    %\n",
    "\n",
    "    X_rec = Z * U(:, 1:K)';\n",
    "\n",
    "    % =============================================================\n",
    "\n",
    "end\n",
    "\n",
    "X_rec  = recoverData(Z, U, K);\n",
    "fprintf('Approximation of the first example: %f %f\\n', X_rec(1, 1), X_rec(1, 2));\n",
    "fprintf('\\n(this value should be about  -1.047419 -1.047419)\\n\\n');\n",
    "\n",
    "%  Draw lines connecting the projected points to the original points\n",
    "%hold on;\n",
    "%plot(X_rec(:, 1), X_rec(:, 2), 'ro');\n",
    "%for i = 1:size(X_norm, 1)\n",
    "%    drawLine(X_norm(i,:), X_rec(i,:), '--k', 'LineWidth', 1);\n",
    "%end\n",
    "%hold off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading face dataset.\n",
      "\n",
      "warning: load: '/Users/jarmo/development/projects/stanford-machine-learning-course/exercises/ex7/ex7faces.mat' found by searching load path\n"
     ]
    }
   ],
   "source": [
    "%% =============== Part 4: Loading and Visualizing Face Data =============\n",
    "%  We start the exercise by first loading and visualizing the dataset.\n",
    "%  The following code will load the dataset into your environment\n",
    "%\n",
    "fprintf('\\nLoading face dataset.\\n\\n');\n",
    "\n",
    "%  Load Face dataset\n",
    "load ('ex7faces.mat')\n",
    "\n",
    "%  Display the first 100 faces in the dataset\n",
    "%displayData(X(1:100, :));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running PCA on face dataset.\n",
      "(this mght take a minute or two ...)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% =========== Part 5: PCA on Face Data: Eigenfaces  ===================\n",
    "%  Run PCA and visualize the eigenvectors which are in this case eigenfaces\n",
    "%  We display the first 36 eigenfaces.\n",
    "%\n",
    "fprintf(['\\nRunning PCA on face dataset.\\n' ...\n",
    "         '(this mght take a minute or two ...)\\n\\n']);\n",
    "\n",
    "%  Before running PCA, it is important to first normalize X by subtracting \n",
    "%  the mean value from each feature\n",
    "[X_norm, mu, sigma] = featureNormalize(X);\n",
    "\n",
    "%  Run PCA\n",
    "[U, S] = pca(X_norm);\n",
    "\n",
    "%  Visualize the top 36 eigenvectors found\n",
    "%displayData(U(:, 1:36)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimension reduction for face dataset.\n",
      "\n",
      "The projected data Z has a size of: \n",
      "5000 100 \n"
     ]
    }
   ],
   "source": [
    "%% ============= Part 6: Dimension Reduction for Faces =================\n",
    "%  Project images to the eigen space using the top k eigenvectors \n",
    "%  If you are applying a machine learning algorithm \n",
    "fprintf('\\nDimension reduction for face dataset.\\n\\n');\n",
    "\n",
    "K = 100;\n",
    "Z = projectData(X_norm, U, K);\n",
    "\n",
    "fprintf('The projected data Z has a size of: ')\n",
    "fprintf('%d ', size(Z));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizing the projected (reduced dimension) faces.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% ==== Part 7: Visualization of Faces after PCA Dimension Reduction ====\n",
    "%  Project images to the eigen space using the top K eigen vectors and \n",
    "%  visualize only using those K dimensions\n",
    "%  Compare to the original input, which is also displayed\n",
    "\n",
    "fprintf('\\nVisualizing the projected (reduced dimension) faces.\\n\\n');\n",
    "\n",
    "K = 100;\n",
    "X_rec  = recoverData(Z, U, K);\n",
    "\n",
    "% Display normalized data\n",
    "%subplot(1, 2, 1);\n",
    "%displayData(X_norm(1:100,:));\n",
    "%title('Original faces');\n",
    "%axis square;\n",
    "%\n",
    "%% Display reconstructed data from only k eigenfaces\n",
    "%subplot(1, 2, 2);\n",
    "%displayData(X_rec(1:100,:));\n",
    "%title('Recovered faces');\n",
    "%axis square;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% === Part 8(a): Optional (ungraded) Exercise: PCA for Visualization ===\n",
    "%  One useful application of PCA is to use it to visualize high-dimensional\n",
    "%  data. In the last K-Means exercise you ran K-Means on 3-dimensional \n",
    "%  pixel colors of an image. We first visualize this output in 3D, and then\n",
    "%  apply PCA to obtain a visualization in 2D.\n",
    "\n",
    "close all; close all; clc\n",
    "\n",
    "% Re-load the image from the previous exercise and run K-Means on it\n",
    "% For this to work, you need to complete the K-Means assignment first\n",
    "A = double(imread('bird_small.png'));\n",
    "\n",
    "% If imread does not work for you, you can try instead\n",
    "%   load ('bird_small.mat');\n",
    "\n",
    "A = A / 255;\n",
    "img_size = size(A);\n",
    "X = reshape(A, img_size(1) * img_size(2), 3);\n",
    "K = 16; \n",
    "max_iters = 10;\n",
    "initial_centroids = kMeansInitCentroids(X, K);\n",
    "[centroids, idx] = runkMeans(X, initial_centroids, max_iters);\n",
    "\n",
    "%  Sample 1000 random indexes (since working with all the data is\n",
    "%  too expensive. If you have a fast computer, you may increase this.\n",
    "sel = floor(rand(1000, 1) * size(X, 1)) + 1;\n",
    "\n",
    "%  Setup Color Palette\n",
    "palette = hsv(K);\n",
    "colors = palette(idx(sel), :);\n",
    "\n",
    "%  Visualize the data and centroid memberships in 3D\n",
    "figure;\n",
    "scatter3(X(sel, 1), X(sel, 2), X(sel, 3), 10, colors);\n",
    "title('Pixel dataset plotted in 3D. Color shows centroid memberships');\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%% === Part 8(b): Optional (ungraded) Exercise: PCA for Visualization ===\n",
    "% Use PCA to project this cloud to 2D for visualization\n",
    "\n",
    "% Subtract the mean to use PCA\n",
    "[X_norm, mu, sigma] = featureNormalize(X);\n",
    "\n",
    "% PCA and project the data to 2D\n",
    "[U, S] = pca(X_norm);\n",
    "Z = projectData(X_norm, U, 2);\n",
    "\n",
    "% Plot in 2D\n",
    "figure;\n",
    "plotDataPoints(Z(sel, :), idx(sel), K);\n",
    "title('Pixel dataset plotted in 2D, using PCA for dimensionality reduction');\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "0.16.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
